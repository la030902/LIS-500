<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Teachable Machine</title>
  <link rel="stylesheet" href="stylepage.css" />

  <!-- Required Libraries for p5.js + ml5.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.dom.min.js"></script>
  <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>
  

  <style>
    canvas {
      display: block;
      margin: 0 auto;
      margin-top: 30px;
    }
    #canvas-container {
      text-align: center;
    }
  </style>
</head>
<body>
  <nav>
    <div class="logo">LIS 500 Project</div>
    <ul class="nav-links">
      <li><a href="index.html">Welcome</a></li>
      <li><a href="about us.html">About</a></li>
      <li><a href="resource.html">Resources</a></li>
      <li><a href="tech hero.html">Tech Hero</a></li>
      <li><a href="teachable.html">Teachable Machines</a></li>
    </ul>
  </nav>

<section class="introduction">
  <div class="intro-container">
    <h2>Introduction</h2>
    <p>
      For this project, we built a simple web app that can recognize traffic signs using a webcam. We trained a model with Google’s Teachable Machine to identify three types of signs—stop, street, and yield—and then integrated it into a webpage using HTML, CSS, and JavaScript.
    </p>
    <p>
      At first, it just seemed like a fun, hands-on way to learn about machine learning. But as we tested it more, we started noticing how easily the model could get confused. Things like lighting, camera angle, or even slight differences between signs affected the results. That made us realize how tricky it is for machines to interpret the real world, even when the task seems straightforward.
    </p>
    <p>
      It also got us thinking about the bigger picture. In <em>Unmasking AI</em>, Joy Buolamwini talks about how biased data in facial recognition can lead to unfair or inaccurate outcomes. While our project was much smaller in scale, we still saw how important it is to have a diverse and well-balanced dataset. Our model sometimes misread signs simply because it hadn’t seen enough examples from different situations.
    </p>
    <p>
      Overall, this project gave us a deeper look into how machine learning works—and how much it relies on the data we feed it. Teaching a computer to recognize traffic signs sounds easy, but it turns out there’s a lot more going on behind the scenes.
    </p>

    <h2>Watch Our Model</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/s0aZmofq8Pc" 
title="YouTube video player" frameborder="0" allowfullscreen></iframe>
<section class="lessonslearned&process">

    <h2>Process & Lessons Learned</h2>
    <p>
    One key takeaway was the importance of data quality and diversity. Our model’s performance was only as good as the dataset we trained it on. Since we only used a relatively small number of images, and most were taken under similar conditions, the model struggled to generalize to new environments. For example, it might perform well when identifying a stop sign held directly in front of the webcam under good lighting, but fail completely if the same sign was slightly tilted or partially obscured. This limitation underscored just how dependent machine learning models are on the data they’re trained with. A well-performing model isn't just about algorithms—it's about feeding those algorithms with varied, representative, and high-quality data.
These technical challenges also made us reflect on some of the broader implications of machine learning. In Unmasking AI, Joy Buolamwini discusses the dangers of biased data in facial recognition systems and how those biases can lead to unfair or even harmful outcomes, particularly for people from marginalized communities. While our project was much smaller in scope, the principle still applied. We witnessed firsthand how a limited dataset could lead to inaccurate predictions. If our small model could be thrown off by something as minor as a dim room or a slightly different angle, it’s not hard to imagine how larger systems might make serious errors if not trained with comprehensive, inclusive datasets.
</p>
    <p>
  This also got us thinking about trust and accountability in AI systems. In many real-world applications—like self-driving cars, medical diagnostics, or facial recognition in law enforcement—there’s a tendency to trust the output of a machine simply because it’s assumed to be objective. But our project showed that machine learning models are only as objective as the data and assumptions behind them. If a model hasn’t been trained on enough examples—or if those examples are skewed in some way—it can produce biased or unreliable results. And if those results are used in critical decision-making processes, the consequences can be serious.
From a technical standpoint, the project was also a great learning experience in terms of web development and integrating machine learning into a front-end interface. We got to practice writing clean, responsive HTML and CSS, and learned how to use JavaScript to handle model loading, webcam input, and real-time predictions. This helped bridge the gap between abstract concepts in AI and the practical skills needed to build usable tools. 
        One of the most rewarding parts was seeing the application come to life: pointing a traffic sign at the webcam and seeing the prediction update in real time made the concepts feel tangible and exciting.
</p>
    <p>
  The iterative process of testing and debugging was also instructive. At times, we had to go back and retrain the model with new data, adjust the JavaScript to better handle loading issues, or tweak the CSS to make the UI more user-friendly. These moments helped us develop problem-solving skills and reinforced the idea that developing AI applications is rarely a one-and-done task. It requires constant refinement, feedback, and a willingness to adapt based on what you learn.
In the end, this project gave us a much deeper appreciation for both the potential and the limitations of machine learning. Teaching a computer to recognize traffic signs may sound like a straightforward task, but as we learned, there are many layers involved—from collecting and curating data, to handling edge cases, to designing an intuitive user interface. It also reminded us that machine learning is not magic—it’s a tool that reflects the data and intentions of the people who build it. As developers and future practitioners in the tech field, it’s crucial for us to be mindful of how these systems are trained and used, and to always consider the social and ethical implications of the tools we create.
What started as a small, experimental project ended up sparking meaningful conversations about accuracy, bias, and the importance of inclusive data in machine learning. It showed us the importance of critical thinking and responsibility in tech, and left us with valuable skills and insights we’ll carry into future projects and careers.
</section>


  
  <a href="classifier.html" class="button">Try our model!</a>

 
 <section id="about">
  <div class="container">
    <h2>Project Statements</h2>
    <ul class="buttons">
      <li><a href="#Joan" class="profile-btn">Joan Lee</a></li>
      <li><a href="#Madison" class="profile-btn">Madison Svuem</a></li>
      <li><a href="#Ann" class="profile-btn">Ann Teoh</a></li>
    </ul>  
     <div class = "profile" id="Joan">
      <p>As we worked on training our own image recognition model using Google’s Teachable Machine, I started to notice something unsettling, not just in the way the model reacted to our inputs, but in what that reaction showed. The process made it clear how dependent AI is on the data it's given, and how easily it can fail when that data lacks variety or context. At the same time, reading Unmasking AI added a whole new layer to what we were doing. Her experiences with biased AI systems and the broader structural issues behind them helped me see that this isn’t just about technology. It’s about who gets represented, who doesn’t, and how that affects the outcomes AI produces. Even though our project was small, it made me start questioning the systems around me and the trust we put in algorithms that are, at their core, shaped by human choices.
      <p>Our goal for this project was to train an image classifier that could recognize three types of traffic signs: Stop, Yield, and Street. When we first started this project, I didn’t expect it to make me think so much about the real-world impact of AI. I thought we were just going to train a computer to recognize traffic signs using Google’s Teachable Machine, and that would be it. But as soon as we began collecting data and actually training our model, I realized there was a lot more going on behind the scenes. Every choice we made, from how we captured images to how consistent the lighting was, affected the results. It wasn’t just about the tech, but about how the tech learns. At the same time, we were reading Unmasking AI, and everything we learned from the book started to connect with what we were seeing firsthand. This project showed me how complicated and sensitive machine learning actually is, and how important it is to be thoughtful about it.
      <p>This project ended up being more than just a technical task. It gave us a chance to reflect on the bigger questions around machine learning, especially the concerns Joy Buolamwini raises in Unmasking AI. Even though our project was on a small scale, the same issues around bias, data quality, and accuracy were present throughout the process.
      <p>Originally, I planned to take our laptops outside and use the webcam on the teachable machine website to capture real images of traffic signs in our neighborhood. But I quickly ran into a problem. I realized that carrying the laptop around and trying to angle the webcam toward a street sign was awkward and not very effective since it is heavy and big. So instead, I came up with a workaround. I took photos of Stop, Yield, and Street signs, then pulled those photos up on our phone screens. To make the signs large enough and clear enough, I zoomed in and held our phones up to the webcam while capturing the training data.
      This method actually worked better than I thought  in some ways. It allowed me to stay in one place and collect a decent number of images quickly. But it also created a new set of challenges. The model had a hard time recognizing the signs when there was glare on the phone screen or if the image wasn’t centered just right. Even slight differences in how I held the phones made the predictions less reliable. Sometimes, the model would completely misidentify a Stop sign as a Yield sign just because of a subtle shadow or angle shift.
      <p>After enough training images were collected, we tested the model to see how well it could recognize the signs in real time. The results were hit-or-miss. Occasionally it would work perfectly, but other times it would flicker between two labels.
      This experience reminded us that machine learning is very sensitive to the conditions under which it’s trained. A model is only as good as the data it learns from. If it’s trained on a narrow set of examples (like screenshots of street signs on phone screens), it will struggle when it sees anything even slightly different.
      <p>As we worked on this, we kept thinking back to Unmasking AI and its content of experiences with biased facial recognition systems. One of the big takeaways from the book is that AI models can’t be trusted to be fair or accurate just because they’re based on data. The way data is collected, who is included or excluded, and what assumptions are built into the system. Even though we weren't working with facial recognition, it struggled when the lighting changed or when it saw a sign it hadn't rained on. This might seem like a small issue, but it actually mirrors much bigger problems like how facial recognition systems often misidentify people with darker skins because they weren't well represented in the training data.
      <p>Looking back on the project, I think what surprised me most was how much it made me think—not just about how to train a machine, but about the bigger stuff that goes into AI. We started with this simple idea: let’s get a computer to recognize traffic signs. But the further we got, the more I realized how many little decisions we had to make, and how those decisions really shaped what the model could do. I also learned that building a model isn’t just about writing code, but it’s about making choices: what data to use, how to present it, how much to collect, and how to test it. Every step in that process affects the final result. It made me think more critically about the kinds of AI tools we encounter every day and how much trust I place in them.
      <p>I used to think of AI as this super advanced tech that just works. But doing this project showed me that it’s actually kind of fragile. Our model would work fine one second and then totally glitch out the next just because we moved the phone slightly or the lighting changed. And that’s just traffic signs—imagine that kind of instability in something serious like facial recognition or law enforcement.
      <p>Reading Unmasking AI while doing this made everything hit harder. The author talks a lot about bias in AI, and even though our project was small, we could already see how limited training data messes with the outcomes. 
I now know that every dataset, every decision, every tool we build reflects something. And going forward, I want to be the kind of person who pays attention to that and questions how things are made, not just if they work. </p>
        </div>
        
        <div class= "profile" id = "Madison"> 

For this project, we built a simple web application that uses a webcam to recognize traffic signs in real time. Our main goal was to create an interactive demo showing how machine learning (ML) can be applied to computer vision tasks, even within a browser. We aimed to make something functional, educational, and hands-on to explore the potential and challenges of real-time image classification.

To get started, we used **Google’s Teachable Machine**, an intuitive platform that allows users to train ML models without writing any code. It supports image, audio, and pose recognition, and is ideal for beginners. We trained our model to recognize three types of traffic signs: **stop signs**, **yield signs**, and **street signs**. We captured multiple images of each sign type using a webcam, making sure to include some variation in angles, lighting, and distance.

Once trained, we exported the model and integrated it into a simple webpage using HTML, CSS, and JavaScript. The webcam feed is shown on the page, and every frame is passed to the model for classification. 
        <br>
        
        The predicted sign and a confidence score are then displayed on the screen, giving users real-time feedback as they test different signs in front of the camera.

Initially, the project was fun and felt like a straightforward way to understand ML. Seeing the model successfully recognize printed traffic signs was satisfying and gave us confidence in the concept. However, as we tested further, we quickly realized how sensitive the model was to its environment.

Small changes in lighting, such as glare or shadows, often caused the model to misclassify signs. Camera angle and distance also played a big role—if the sign wasn’t positioned just right, accuracy dropped. We also discovered that **visual variations between signs of the same type—like font changes or faded colors—could throw off the model. These limitations made it clear that building a reliable vision system is more complicated than it seems.

<br>
        Through this process, we gained a deeper understanding of the **challenges in machine learning and computer vision. Real-world data is unpredictable, and models need a lot of high-quality, diverse training data to be robust. Even with a tool as accessible as Teachable Machine, we saw the importance of careful testing, ongoing refinement, and understanding a model’s weaknesses.

This project also highlighted the value of **transparent design**. By including confidence scores and keeping the interface simple, we made it easier to interpret the model’s predictions. Users could see not just what the model thought, but how sure it was, which helped us and others understand how well the model was performing in different scenarios.

In the end, our traffic sign recognition app served as a practical and eye-opening experience. It helped us appreciate both the potential of machine learning and the complexity behind getting models to work well in dynamic, real-world situations. While the tool we created is basic, it reflects many core lessons of machine learning—data matters, context matters, and even small projects can teach big ideas.
      </p>
      </div>

      <div class="profile" id = "Ann">
        <p>As part of a group project, we set out to develop a basic web application that uses machine learning to recognize traffic signs in real time.  The main objective was to create an interactive demonstration that would highlight machine learning (ML) applications in computer vision while simultaneously acting as a teaching tool to clarify the field's possibilities and difficulties, especially when considering practical implementations. As it complemented important ideas from Joy Buolamwini's "Unmasking AI," a book that has greatly impacted my comprehension of artificial intelligence and its societal ramifications, this project was very intriguing.</p>
        <p>I was mostly thinking on the technical parts of the project when we started, such as how to train a model to detect different traffic signs.  The complexities of machine learning, such as data collecting, model training, and real-time prediction, excited me.  But I quickly realized that this task would set me on a course of critical inquiry regarding the ethical issues surrounding AI, which Buolamwini skilfully addresses in her book.  I started thinking about how the calibre and variety of the training data directly affect the potential and constraints of the models we create, highlighting the fact that artificial intelligence is more about the data and the human choices that form it than it is about the technology itself.</p>
        <p>We started our experiment by using Google's Teachable Machine, an approachable platform that enables users to train machine learning models without requiring a deep understanding of programming.  Three distinct categories of traffic signs—stop, yield, and street signs—were the subject of our investigation.  To generate a solid dataset, the first step in the image collection process was taking numerous pictures with a webcam, making sure to include a variety of perspectives, lighting situations, and distances. </p>
        <p>Curiously, I ran into some technical difficulties as soon as I started the data collection step.  I had originally intended to take my laptop outside and snap pictures straight from the real world.  But carrying a heavy laptop about was inconvenient.  I chose to utilize my smartphone as a workaround, displaying pictures of the signs and using the webcam to record those movies. This method originally proved to be quite successful.  However, as we went along, it became clear that this approach had several drawbacks. For example, screen glare and minor positioning variations could cause noise to enter the model and result in incorrect classifications.</p>
        <P>These issues in real time started to parallel the things I was seeing in Buolamwini's "Unmasking AI."  The book emphasizes the difficulties faced by machine learning models, such as facial recognition software, when they are trained on data that is not representative or diverse.  I saw that although though my model was made to identify traffic signs, the same concepts were influencing its performance—the caliber and variety of the training data greatly influenced its effectiveness</P>
        <p>After our model had been adequately trained, we used HTML, CSS, and JavaScript to incorporate it into a basic webpage.  With the model categorizing the signs in real-time and giving feedback through anticipated labels and confidence scores, the finished result showed a live webcam stream.  At first, the accomplishment of identifying indicators was thrilling.  When the model correctly recognized a yield sign or a stop sign, there was, in fact, a noticeable sense of accomplishment</p>
        <p>But as we started properly testing the program, the euphoria was short-lived.  It became abundantly evident how sensitive our model was to the constantly shifting external circumstances.  Misclassification, sometimes even amongst symptoms of the same type, could be easily caused by little changes in lighting or camera position.  A well-lit sign at a specific angle, for example, would be accurately recognized; nevertheless, a small change in position or ambient light would frequently cause the model to flicker between labels or fail completely. </p>
        <p>As I think back on my experiences working on this project, I've learned a lot about how ethics and technology interact.  A potent reminder that machine learning is not just about algorithms, but also much depends on the calibre of the data utilized and the underlying presumptions ingrained in these systems, was provided by the discovery that our AI model would not work unless trained on a diverse dataset. </p>
        <p>Working on this project also made me think about how many AI technologies are viewed by many as complex systems that many people blindly trust without realizing the biases or flaws, they may include.  Buolamwini's views on AI system accountability have strengthened my determination to challenge the status quo.  I now feel obligated to investigate the subtleties of AI systems' evolution rather than taking them at face value.</p>
        <p>Transparency served as a compass for our effort.  Our application's incorporation of confidence scores gave users access to both the anticipated label and the model's level of certainty regarding that classification.  I think that this straightforward functionality can help consumers better comprehend the potential and constraints of artificial intelligence.  It can spark discussions about how crucial it is to assess the instruments we create critically and the effects these technologies have on larger social structures.</p>
        <p>In conclusion, creating our traffic sign recognition system has opened our eyes to the complicated realm of AI ethics and its uses, going beyond a simple investigation of machine learning.  As we worked through the technological difficulties and reexamined our choices, I kept going back to the concepts discussed in "Unmasking AI."  My critical evaluation of machine learning's potential applications and significant societal effects was prompted by this study. </p>
        <p>As I continue my technological journey are the priceless lessons I've learned about accountability, openness, and moral interaction with AI systems.  Even if the road ahead may be full of unanticipated obstacles, I now have a mindset that puts knowledge above algorithmic trust.  Although it began as a project about traffic signs, our straightforward web application has developed into a forum for thoughtful discussion of the social roles and obligations of technology developers.</p>
      </div>
  </div>
</section>

<script>
  function showDescription(person) {
    document.querySelectorAll('.description').forEach(desc => desc.style.display = 'none');
    document.getElementById(person).style.display = 'block';
  }
</script>


  <script src="sketch.js"></script>

  <footer>
    <p>&copy; Joan Lee, Madison Sveum, Ann Teoh. LIS 500 Project2</p>
  </footer>


</body>
</html>
