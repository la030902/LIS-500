<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Teachable Machine</title>
  <link rel="stylesheet" href="stylepage.css" />

  <!-- Required Libraries for p5.js + ml5.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.dom.min.js"></script>
  <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>
  

  <style>
    canvas {
      display: block;
      margin: 0 auto;
      margin-top: 30px;
    }
    #canvas-container {
      text-align: center;
    }
  </style>
</head>
<body>
  <nav>
    <div class="logo">LIS 500 Project</div>
    <ul class="nav-links">
      <li><a href="index.html">Welcome</a></li>
      <li><a href="about us.html">About</a></li>
      <li><a href="resource.html">Resources</a></li>
      <li><a href="tech hero.html">Tech Hero</a></li>
      <li><a href="teachable.html">Teachable Machines</a></li>
    </ul>
  </nav>

<section class="introduction">
  <div class="intro-container">
    <h2>Introduction</h2>
    <p>
      dFor this project, we built a simple web app that can recognize traffic signs using a webcam. We trained a model with Google’s Teachable Machine to identify three types of signs—stop, street, and yield—and then integrated it into a webpage using HTML, CSS, and JavaScript.
    </p>
    <p>
      At first, it just seemed like a fun, hands-on way to learn about machine learning. But as we tested it more, we started noticing how easily the model could get confused. Things like lighting, camera angle, or even slight differences between signs affected the results. That made us realize how tricky it is for machines to interpret the real world, even when the task seems straightforward.
    </p>
    <p>
      It also got us thinking about the bigger picture. In <em>Unmasking AI</em>, Joy Buolamwini talks about how biased data in facial recognition can lead to unfair or inaccurate outcomes. While our project was much smaller in scale, we still saw how important it is to have a diverse and well-balanced dataset. Our model sometimes misread signs simply because it hadn’t seen enough examples from different situations.
    </p>
    <p>
      Overall, this project gave us a deeper look into how machine learning works—and how much it relies on the data we feed it. Teaching a computer to recognize traffic signs sounds easy, but it turns out there’s a lot more going on behind the scenes.
    </p>
  </div>
</section>


  
  <section id="canvas-container">
    <h2>Live Video Classifier</h2>
    <p id="label-display">Label: <span id="label">waiting...</span></p>
  </section>
 
 <section id="about">
  <div class="container">
    <h2>Project Statements</h2>
     <div class="buttons">
      <button class="profile-btn" onclick="showDescription('joan')">Joan Lee</button>
      <button class="profile-btn" onclick="showDescription('madison')">Madison Sveum</button>
      <button class="profile-btn" onclick="showDescription('ann')">Ann Teoh</button>
    </div>
     <div id="description">
      <p id="joan" class="description">As we worked on training our own image recognition model using Google’s Teachable Machine, I started to notice something unsettling, not just in the way the model reacted to our inputs, but in what that reaction showed. The process made it clear how dependent AI is on the data it's given, and how easily it can fail when that data lacks variety or context. At the same time, reading Unmasking AI added a whole new layer to what we were doing. Her experiences with biased AI systems and the broader structural issues behind them helped me see that this isn’t just about technology. It’s about who gets represented, who doesn’t, and how that affects the outcomes AI produces. Even though our project was small, it made me start questioning the systems around me and the trust we put in algorithms that are, at their core, shaped by human choices.
Our goal for this project was to train an image classifier that could recognize three types of traffic signs: Stop, Yield, and Street. When we first started this project, I didn’t expect it to make me think so much about the real-world impact of AI. I thought we were just going to train a computer to recognize traffic signs using Google’s Teachable Machine, and that would be it. But as soon as we began collecting data and actually training our model, I realized there was a lot more going on behind the scenes. Every choice we made, from how we captured images to how consistent the lighting was, affected the results. It wasn’t just about the tech, but about how the tech learns. At the same time, we were reading Unmasking AI, and everything we learned from the book started to connect with what we were seeing firsthand. This project showed me how complicated and sensitive machine learning actually is, and how important it is to be thoughtful about it.
This project ended up being more than just a technical task. It gave us a chance to reflect on the bigger questions around machine learning, especially the concerns Joy Buolamwini raises in Unmasking AI. Even though our project was on a small scale, the same issues around bias, data quality, and accuracy were present throughout the process.
Originally, I planned to take our laptops outside and use the webcam on the teachable machine website to capture real images of traffic signs in our neighborhood. But I quickly ran into a problem. I realized that carrying the laptop around and trying to angle the webcam toward a street sign was awkward and not very effective since it is heavy and big. So instead, I came up with a workaround. I took photos of Stop, Yield, and Street signs, then pulled those photos up on our phone screens. To make the signs large enough and clear enough, I zoomed in and held our phones up to the webcam while capturing the training data.
This method actually worked better than I thought  in some ways. It allowed me to stay in one place and collect a decent number of images quickly. But it also created a new set of challenges. The model had a hard time recognizing the signs when there was glare on the phone screen or if the image wasn’t centered just right. Even slight differences in how I held the phones made the predictions less reliable. Sometimes, the model would completely misidentify a Stop sign as a Yield sign just because of a subtle shadow or angle shift.
After enough training images were collected, we tested the model to see how well it could recognize the signs in real time. The results were hit-or-miss. Occasionally it would work perfectly, but other times it would flicker between two labels.
This experience reminded us that machine learning is very sensitive to the conditions under which it’s trained. A model is only as good as the data it learns from. If it’s trained on a narrow set of examples (like screenshots of street signs on phone screens), it will struggle when it sees anything even slightly different.
As we worked on this, we kept thinking back to Unmasking AI and its content of experiences with biased facial recognition systems. One of the big takeaways from the book is that AI models can’t be trusted to be fair or accurate just because they’re based on data. The way data is collected, who is included or excluded, and what assumptions are built into the system. 

Even though we weren;t working with facial recognition, it struggled when the lighting changed or when it saw a sign it hadn't rained on. This might seem like a small issue, but it actually mirrors much bigger problems like how facial recognition systems often misidentify people with darker skins because they weren't well represented in the training data.
Looking back on the project, I think what surprised me most was how much it made me think—not just about how to train a machine, but about the bigger stuff that goes into AI. We started with this simple idea: let’s get a computer to recognize traffic signs. But the further we got, the more I realized how many little decisions we had to make, and how those decisions really shaped what the model could do. I also learned that building a model isn’t just about writing code, but it’s about making choices: what data to use, how to present it, how much to collect, and how to test it. Every step in that process affects the final result. It made me think more critically about the kinds of AI tools we encounter every day and how much trust I place in them.
I used to think of AI as this super advanced tech that just works. But doing this project showed me that it’s actually kind of fragile. Our model would work fine one second and then totally glitch out the next just because we moved the phone slightly or the lighting changed. And that’s just traffic signs—imagine that kind of instability in something serious like facial recognition or law enforcement.
Reading Unmasking AI while doing this made everything hit harder. The author talks a lot about bias in AI, and even though our project was small, we could already see how limited training data messes with the outcomes. 
I now know that every dataset, every decision, every tool we build reflects something. And going forward, I want to be the kind of person who pays attention to that and questions how things are made, not just if they work. </p>

        
      <p id="madison" class="description"> For this project, we developed a basic web application that uses a webcam to recognize traffic signs in real time. Our primary goal was to create an interactive and functional demonstration of how machine learning can be applied to computer vision tasks, even in a simple browser-based environment. We began by training a model with Google’s Teachable Machine, a user-friendly platform that allows anyone to train image, sound, or pose recognition models using a webcam or uploaded files. Specifically, we trained our model to detect three types of common traffic signs: stop signs, street signs, and yield signs. Once the training was complete, we exported the model and embedded it into a webpage using a combination of HTML, CSS, and JavaScript.
Initially, the project seemed like a fun, hands-on way to explore the basics of machine learning and see how models can interact with the real world through a camera. Using Teachable Machine made the training process very approachable. We collected several images for each category of traffic sign, trained the model, and set it up to run in the browser so that it could classify whatever the webcam was seeing. The interface was designed to be simple and clear, displaying the predicted class on the screen along with a confidence score, so we could test different scenarios and observe how the model responded in real time.
However, as we continued to test and fine-tune the application, we began to notice some significant limitations. It quickly became apparent that the model’s performance was highly sensitive to a variety of environmental factors. For instance, changes in lighting—such as shadows or glare—could lead the model to misclassify a sign. Similarly, the angle of the camera or the distance from the object made a big difference in whether the sign was recognized correctly. Even slight variations between different signs of the same type (such as different fonts or colors on a stop sign) sometimes confused the model. These issues made us realize that, while it might seem simple to teach a computer to recognize traffic signs, the reality is far more complex. The physical world is full of subtle nuances, and computers aren’t naturally good at dealing with those without a lot of help.
This realization opened the door to a much deeper understanding of the challenges behind machine learning and computer vision.
        <br>
        One key takeaway was the importance of data quality and diversity. Our model’s performance was only as good as the dataset we trained it on. Since we only used a relatively small number of images, and most were taken under similar conditions, the model struggled to generalize to new environments. For example, it might perform well when identifying a stop sign held directly in front of the webcam under good lighting, but fail completely if the same sign was slightly tilted or partially obscured. This limitation underscored just how dependent machine learning models are on the data they’re trained with. A well-performing model isn't just about algorithms—it's about feeding those algorithms with varied, representative, and high-quality data.
These technical challenges also made us reflect on some of the broader implications of machine learning. 
        In Unmasking AI, Joy Buolamwini discusses the dangers of biased data in facial recognition systems and how those biases can lead to unfair or even harmful outcomes, particularly for people from marginalized communities. While our project was much smaller in scope, the principle still applied. We witnessed firsthand how a limited dataset could lead to inaccurate predictions. If our small model could be thrown off by something as minor as a dim room or a slightly different angle, it’s not hard to imagine how larger systems might make serious errors if not trained with comprehensive, inclusive datasets.
<br>
        This also got us thinking about trust and accountability in AI systems. In many real-world applications—like self-driving cars, medical diagnostics, or facial recognition in law enforcement—there’s a tendency to trust the output of a machine simply because it’s assumed to be objective. But our project showed that machine learning models are only as objective as the data and assumptions behind them. If a model hasn’t been trained on enough examples—or if those examples are skewed in some way—it can produce biased or unreliable results. And if those results are used in critical decision-making processes, the consequences can be serious.
From a technical standpoint, the project was also a great learning experience in terms of web development and integrating machine learning into a front-end interface. We got to practice writing clean, responsive HTML and CSS, and learned how to use JavaScript to handle model loading, webcam input, and real-time predictions. This helped bridge the gap between abstract concepts in AI and the practical skills needed to build usable tools. 
        One of the most rewarding parts was seeing the application come to life: pointing a traffic sign at the webcam and seeing the prediction update in real time made the concepts feel tangible and exciting.
<br>
        The iterative process of testing and debugging was also instructive. At times, we had to go back and retrain the model with new data, adjust the JavaScript to better handle loading issues, or tweak the CSS to make the UI more user-friendly. These moments helped us develop problem-solving skills and reinforced the idea that developing AI applications is rarely a one-and-done task. It requires constant refinement, feedback, and a willingness to adapt based on what you learn.
In the end, this project gave us a much deeper appreciation for both the potential and the limitations of machine learning. Teaching a computer to recognize traffic signs may sound like a straightforward task, but as we learned, there are many layers involved—from collecting and curating data, to handling edge cases, to designing an intuitive user interface. It also reminded us that machine learning is not magic—it’s a tool that reflects the data and intentions of the people who build it. As developers and future practitioners in the tech field, it’s crucial for us to be mindful of how these systems are trained and used, and to always consider the social and ethical implications of the tools we create.
What started as a small, experimental project ended up sparking meaningful conversations about accuracy, bias, and the importance of inclusive data in machine learning. It showed us the importance of critical thinking and responsibility in tech, and left us with valuable skills and insights we’ll carry into future projects and careers.
</p>
      <p id="ann" class="description">Hi, I’m Ann Teoh from Kuala Lumpur, Malaysia. I am a senior majoring in Information Science with a minor in Digital Studies at UW. I decided to take this course based on a recommendation from a friend who found it interesting. Initially, I enrolled to improve my web development skills, particularly in HTML and CSS. However, as the semester progressed, I found the course even more engaging than I expected. It not only deepens my technical skills but also connects to the ethical implications of technology—particularly how those in power influence technology and how issues of race and discrimination manifest in digital spaces. I want to become more aware of biases in technology and learn how to create more inclusive digital environments.</p>
    </div>
  </div>
</section>

<script>
  function showDescription(person) {
    document.querySelectorAll('.description').forEach(desc => desc.style.display = 'none');
    document.getElementById(person).style.display = 'block';
  }
</script>

  <script>
  // Your Teachable Machine model URL
  const URL = "https://teachablemachine.withgoogle.com/models/lsBrn0MsP/";

  let model, webcam, labelContainer, maxPredictions;

  // Load the image model and setup the webcam
  async function init() {
    const modelURL = URL + "model.json";
    const metadataURL = URL + "metadata.json";

    // Show loading message
    labelContainer = document.getElementById("label-container");
    labelContainer.innerHTML = "<div>Loading model...</div>";

    // Load the model and metadata
    model = await tmImage.load(modelURL, metadataURL);
    maxPredictions = model.getTotalClasses();

    // Setup webcam
    const flip = true; // whether to flip the webcam
    webcam = new tmImage.Webcam(400, 400, flip); // width, height, flip
    await webcam.setup(); // request access to the webcam
    await webcam.play();
    
    // Append webcam to container
    document.getElementById("webcam-container").appendChild(webcam.canvas);
    
    // Update loading message
    labelContainer.innerHTML = "";
    for (let i = 0; i < maxPredictions; i++) {
      labelContainer.appendChild(document.createElement("div"));
    }
    
    // Start prediction loop
    window.requestAnimationFrame(loop);
  }

  async function loop() {
    webcam.update(); // update the webcam frame
    await predict();
    window.requestAnimationFrame(loop);
  }

  // Run the webcam image through the image model
  async function predict() {
    // Predict can take in an image, video or canvas HTML element
    const prediction = await model.predict(webcam.canvas);
    for (let i = 0; i < maxPredictions; i++) {
      const classPrediction =
        prediction[i].className + ": " + 
        (prediction[i].probability * 100).toFixed(2) + "%";

      // Change label text to emoji and class name based on the prediction
      if (prediction[i].className === 'Street sign') {
        labelContainer.childNodes[i].innerHTML = '🚧 ' + classPrediction;
      } else if (prediction[i].className === 'Yield sign') {
        labelContainer.childNodes[i].innerHTML = '⚠️ ' + classPrediction;
      } else if (prediction[i].className === 'Stop sign') {
        labelContainer.childNodes[i].innerHTML = '🛑 ' + classPrediction;
      } else {
        labelContainer.childNodes[i].innerHTML = classPrediction;
      }
    }
  }

  // Call init to load the model and start the prediction process
  init();
</script>


  <script src="sketch.js"></script>

  <footer>
    <p>&copy; Joan Lee, Madison Sveum, Ann Teoh. LIS 500 Project2</p>
  </footer>


</body>
</html>
