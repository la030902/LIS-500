<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Teachable Machine</title>
  <link rel="stylesheet" href="stylepage.css" />

  <!-- Required Libraries for p5.js + ml5.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.dom.min.js"></script>
  <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>
  

  <style>
    canvas {
      display: block;
      margin: 0 auto;
      margin-top: 30px;
    }
    #canvas-container {
      text-align: center;
    }
  </style>
</head>
<body>
  <nav>
    <div class="logo">LIS 500 Project</div>
    <ul class="nav-links">
      <li><a href="index.html">Welcome</a></li>
      <li><a href="about us.html">About</a></li>
      <li><a href="resource.html">Resources</a></li>
      <li><a href="tech hero.html">Tech Hero</a></li>
      <li><a href="teachable.html">Teachable Machines</a></li>
    </ul>
  </nav>

<section class="introduction">
  <div class="intro-container">
    <h2>Introduction</h2>
    <p>
      dFor this project, we built a simple web app that can recognize traffic signs using a webcam. We trained a model with Google’s Teachable Machine to identify three types of signs—stop, street, and yield—and then integrated it into a webpage using HTML, CSS, and JavaScript.
    </p>
    <p>
      At first, it just seemed like a fun, hands-on way to learn about machine learning. But as we tested it more, we started noticing how easily the model could get confused. Things like lighting, camera angle, or even slight differences between signs affected the results. That made us realize how tricky it is for machines to interpret the real world, even when the task seems straightforward.
    </p>
    <p>
      It also got us thinking about the bigger picture. In <em>Unmasking AI</em>, Joy Buolamwini talks about how biased data in facial recognition can lead to unfair or inaccurate outcomes. While our project was much smaller in scale, we still saw how important it is to have a diverse and well-balanced dataset. Our model sometimes misread signs simply because it hadn’t seen enough examples from different situations.
    </p>
    <p>
      Overall, this project gave us a deeper look into how machine learning works—and how much it relies on the data we feed it. Teaching a computer to recognize traffic signs sounds easy, but it turns out there’s a lot more going on behind the scenes.
    </p>

<section class="lessonslearned&process">

    <h2>Process & Lessons Learned</h2>
 <p>
    One key takeaway was the importance of data quality and diversity. Our model’s performance was only as good as the dataset we trained it on. Since we only used a relatively small number of images, and most were taken under similar conditions, the model struggled to generalize to new environments. For example, it might perform well when identifying a stop sign held directly in front of the webcam under good lighting, but fail completely if the same sign was slightly tilted or partially obscured. This limitation underscored just how dependent machine learning models are on the data they’re trained with. A well-performing model isn't just about algorithms—it's about feeding those algorithms with varied, representative, and high-quality data.
These technical challenges also made us reflect on some of the broader implications of machine learning. 
        In Unmasking AI, Joy Buolamwini discusses the dangers of biased data in facial recognition systems and how those biases can lead to unfair or even harmful outcomes, particularly for people from marginalized communities. While our project was much smaller in scope, the principle still applied. We witnessed firsthand how a limited dataset could lead to inaccurate predictions. If our small model could be thrown off by something as minor as a dim room or a slightly different angle, it’s not hard to imagine how larger systems might make serious errors if not trained with comprehensive, inclusive datasets.
</p>
        This also got us thinking about trust and accountability in AI systems. In many real-world applications—like self-driving cars, medical diagnostics, or facial recognition in law enforcement—there’s a tendency to trust the output of a machine simply because it’s assumed to be objective. But our project showed that machine learning models are only as objective as the data and assumptions behind them. If a model hasn’t been trained on enough examples—or if those examples are skewed in some way—it can produce biased or unreliable results. And if those results are used in critical decision-making processes, the consequences can be serious.
From a technical standpoint, the project was also a great learning experience in terms of web development and integrating machine learning into a front-end interface. We got to practice writing clean, responsive HTML and CSS, and learned how to use JavaScript to handle model loading, webcam input, and real-time predictions. This helped bridge the gap between abstract concepts in AI and the practical skills needed to build usable tools. 
        One of the most rewarding parts was seeing the application come to life: pointing a traffic sign at the webcam and seeing the prediction update in real time made the concepts feel tangible and exciting.
</p>
        The iterative process of testing and debugging was also instructive. At times, we had to go back and retrain the model with new data, adjust the JavaScript to better handle loading issues, or tweak the CSS to make the UI more user-friendly. These moments helped us develop problem-solving skills and reinforced the idea that developing AI applications is rarely a one-and-done task. It requires constant refinement, feedback, and a willingness to adapt based on what you learn.
In the end, this project gave us a much deeper appreciation for both the potential and the limitations of machine learning. Teaching a computer to recognize traffic signs may sound like a straightforward task, but as we learned, there are many layers involved—from collecting and curating data, to handling edge cases, to designing an intuitive user interface. It also reminded us that machine learning is not magic—it’s a tool that reflects the data and intentions of the people who build it. As developers and future practitioners in the tech field, it’s crucial for us to be mindful of how these systems are trained and used, and to always consider the social and ethical implications of the tools we create.
What started as a small, experimental project ended up sparking meaningful conversations about accuracy, bias, and the importance of inclusive data in machine learning. It showed us the importance of critical thinking and responsibility in tech, and left us with valuable skills and insights we’ll carry into future projects and careers.v
 </p>
</section>

<h2>Watch Our Model</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/s0aZmofq8Pc" 
title="YouTube video player" frameborder="0" allowfullscreen></iframe>

  
  <section id="canvas-container">
    <h2>Live Video Classifier</h2>
    <p id="label-display">Label: <span id="label">waiting...</span></p>
  </section>
 
 <section id="about">
  <div class="container">
    <h2>Project Statements</h2>
     <div class="buttons">
      <button class="profile-btn" onclick="showDescription('joan')">Joan Lee</button>
      <button class="profile-btn" onclick="showDescription('madison')">Madison Sveum</button>
      <button class="profile-btn" onclick="showDescription('ann')">Ann Teoh</button>
    </div>
     <div id="description">
      <p id="joan" class="description">As we worked on training our own image recognition model using Google’s Teachable Machine, I started to notice something unsettling, not just in the way the model reacted to our inputs, but in what that reaction showed. The process made it clear how dependent AI is on the data it's given, and how easily it can fail when that data lacks variety or context. At the same time, reading Unmasking AI added a whole new layer to what we were doing. Her experiences with biased AI systems and the broader structural issues behind them helped me see that this isn’t just about technology. It’s about who gets represented, who doesn’t, and how that affects the outcomes AI produces. Even though our project was small, it made me start questioning the systems around me and the trust we put in algorithms that are, at their core, shaped by human choices.
Our goal for this project was to train an image classifier that could recognize three types of traffic signs: Stop, Yield, and Street. When we first started this project, I didn’t expect it to make me think so much about the real-world impact of AI. I thought we were just going to train a computer to recognize traffic signs using Google’s Teachable Machine, and that would be it. But as soon as we began collecting data and actually training our model, I realized there was a lot more going on behind the scenes. Every choice we made, from how we captured images to how consistent the lighting was, affected the results. It wasn’t just about the tech, but about how the tech learns. At the same time, we were reading Unmasking AI, and everything we learned from the book started to connect with what we were seeing firsthand. This project showed me how complicated and sensitive machine learning actually is, and how important it is to be thoughtful about it.
This project ended up being more than just a technical task. It gave us a chance to reflect on the bigger questions around machine learning, especially the concerns Joy Buolamwini raises in Unmasking AI. Even though our project was on a small scale, the same issues around bias, data quality, and accuracy were present throughout the process.
Originally, I planned to take our laptops outside and use the webcam on the teachable machine website to capture real images of traffic signs in our neighborhood. But I quickly ran into a problem. I realized that carrying the laptop around and trying to angle the webcam toward a street sign was awkward and not very effective since it is heavy and big. So instead, I came up with a workaround. I took photos of Stop, Yield, and Street signs, then pulled those photos up on our phone screens. To make the signs large enough and clear enough, I zoomed in and held our phones up to the webcam while capturing the training data.
This method actually worked better than I thought  in some ways. It allowed me to stay in one place and collect a decent number of images quickly. But it also created a new set of challenges. The model had a hard time recognizing the signs when there was glare on the phone screen or if the image wasn’t centered just right. Even slight differences in how I held the phones made the predictions less reliable. Sometimes, the model would completely misidentify a Stop sign as a Yield sign just because of a subtle shadow or angle shift.
After enough training images were collected, we tested the model to see how well it could recognize the signs in real time. The results were hit-or-miss. Occasionally it would work perfectly, but other times it would flicker between two labels.
This experience reminded us that machine learning is very sensitive to the conditions under which it’s trained. A model is only as good as the data it learns from. If it’s trained on a narrow set of examples (like screenshots of street signs on phone screens), it will struggle when it sees anything even slightly different.
As we worked on this, we kept thinking back to Unmasking AI and its content of experiences with biased facial recognition systems. One of the big takeaways from the book is that AI models can’t be trusted to be fair or accurate just because they’re based on data. The way data is collected, who is included or excluded, and what assumptions are built into the system. 

Even though we weren;t working with facial recognition, it struggled when the lighting changed or when it saw a sign it hadn't rained on. This might seem like a small issue, but it actually mirrors much bigger problems like how facial recognition systems often misidentify people with darker skins because they weren't well represented in the training data.
Looking back on the project, I think what surprised me most was how much it made me think—not just about how to train a machine, but about the bigger stuff that goes into AI. We started with this simple idea: let’s get a computer to recognize traffic signs. But the further we got, the more I realized how many little decisions we had to make, and how those decisions really shaped what the model could do. I also learned that building a model isn’t just about writing code, but it’s about making choices: what data to use, how to present it, how much to collect, and how to test it. Every step in that process affects the final result. It made me think more critically about the kinds of AI tools we encounter every day and how much trust I place in them.
I used to think of AI as this super advanced tech that just works. But doing this project showed me that it’s actually kind of fragile. Our model would work fine one second and then totally glitch out the next just because we moved the phone slightly or the lighting changed. And that’s just traffic signs—imagine that kind of instability in something serious like facial recognition or law enforcement.
Reading Unmasking AI while doing this made everything hit harder. The author talks a lot about bias in AI, and even though our project was small, we could already see how limited training data messes with the outcomes. 
I now know that every dataset, every decision, every tool we build reflects something. And going forward, I want to be the kind of person who pays attention to that and questions how things are made, not just if they work. </p>

        
      <p id="madison" class="description"> 

For this project, we built a simple web application that uses a webcam to recognize traffic signs in real time. Our main goal was to create an interactive demo showing how machine learning (ML) can be applied to computer vision tasks, even within a browser. We aimed to make something functional, educational, and hands-on to explore the potential and challenges of real-time image classification.

To get started, we used **Google’s Teachable Machine**, an intuitive platform that allows users to train ML models without writing any code. It supports image, audio, and pose recognition, and is ideal for beginners. We trained our model to recognize three types of traffic signs: **stop signs**, **yield signs**, and **street signs**. We captured multiple images of each sign type using a webcam, making sure to include some variation in angles, lighting, and distance.

Once trained, we exported the model and integrated it into a simple webpage using HTML, CSS, and JavaScript. The webcam feed is shown on the page, and every frame is passed to the model for classification. 
        <br>
        
        The predicted sign and a confidence score are then displayed on the screen, giving users real-time feedback as they test different signs in front of the camera.

Initially, the project was fun and felt like a straightforward way to understand ML. Seeing the model successfully recognize printed traffic signs was satisfying and gave us confidence in the concept. However, as we tested further, we quickly realized how sensitive the model was to its environment.

Small changes in lighting, such as glare or shadows, often caused the model to misclassify signs. Camera angle and distance also played a big role—if the sign wasn’t positioned just right, accuracy dropped. We also discovered that **visual variations between signs of the same type—like font changes or faded colors—could throw off the model. These limitations made it clear that building a reliable vision system is more complicated than it seems.

<br>
        Through this process, we gained a deeper understanding of the **challenges in machine learning and computer vision. Real-world data is unpredictable, and models need a lot of high-quality, diverse training data to be robust. Even with a tool as accessible as Teachable Machine, we saw the importance of careful testing, ongoing refinement, and understanding a model’s weaknesses.

This project also highlighted the value of **transparent design**. By including confidence scores and keeping the interface simple, we made it easier to interpret the model’s predictions. Users could see not just what the model thought, but how sure it was, which helped us and others understand how well the model was performing in different scenarios.

In the end, our traffic sign recognition app served as a practical and eye-opening experience. It helped us appreciate both the potential of machine learning and the complexity behind getting models to work well in dynamic, real-world situations. While the tool we created is basic, it reflects many core lessons of machine learning—data matters, context matters, and even small projects can teach big ideas.
      </p>
      <p id="ann" class="description">Hi, I’m Ann Teoh from Kuala Lumpur, Malaysia. I am a senior majoring in Information Science with a minor in Digital Studies at UW. I decided to take this course based on a recommendation from a friend who found it interesting. Initially, I enrolled to improve my web development skills, particularly in HTML and CSS. However, as the semester progressed, I found the course even more engaging than I expected. It not only deepens my technical skills but also connects to the ethical implications of technology—particularly how those in power influence technology and how issues of race and discrimination manifest in digital spaces. I want to become more aware of biases in technology and learn how to create more inclusive digital environments.</p>
    </div>
  </div>
</section>

<script>
  function showDescription(person) {
    document.querySelectorAll('.description').forEach(desc => desc.style.display = 'none');
    document.getElementById(person).style.display = 'block';
  }
</script>


  <script src="sketch.js"></script>

  <footer>
    <p>&copy; Joan Lee, Madison Sveum, Ann Teoh. LIS 500 Project2</p>
  </footer>


</body>
</html>
