<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Teachable Machine</title>
  <link rel="stylesheet" href="stylepage.css" />

  <!-- Required Libraries for p5.js + ml5.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/addons/p5.dom.min.js"></script>
  <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>
  

  <style>
    canvas {
      display: block;
      margin: 0 auto;
      margin-top: 30px;
    }
    #canvas-container {
      text-align: center;
    }
  </style>
</head>
<body>
  <nav>
    <div class="logo">LIS 500 Project</div>
    <ul class="nav-links">
      <li><a href="index.html">Welcome</a></li>
      <li><a href="about us.html">About</a></li>
      <li><a href="resource.html">Resources</a></li>
      <li><a href="tech hero.html">Tech Hero</a></li>
      <li><a href="teachable.html">Teachable Machines</a></li>
    </ul>
  </nav>

<section class="introduction">
  <div class="intro-container">
    <h2>Introduction</h2>
    <p>
      For this project, we built a simple web app that can recognize traffic signs using a webcam. We trained a model with Google’s Teachable Machine to identify three types of signs—stop, street, and yield—and then integrated it into a webpage using HTML, CSS, and JavaScript.
    </p>
    <p>
      At first, it just seemed like a fun, hands-on way to learn about machine learning. But as we tested it more, we started noticing how easily the model could get confused. Things like lighting, camera angle, or even slight differences between signs affected the results. That made us realize how tricky it is for machines to interpret the real world, even when the task seems straightforward.
    </p>
    <p>
      It also got us thinking about the bigger picture. In <em>Unmasking AI</em>, Joy Buolamwini talks about how biased data in facial recognition can lead to unfair or inaccurate outcomes. While our project was much smaller in scale, we still saw how important it is to have a diverse and well-balanced dataset. Our model sometimes misread signs simply because it hadn’t seen enough examples from different situations.
    </p>
    <p>
      Overall, this project gave us a deeper look into how machine learning works—and how much it relies on the data we feed it. Teaching a computer to recognize traffic signs sounds easy, but it turns out there’s a lot more going on behind the scenes.
    </p>
  </div>
</section>


  
  <section id="canvas-container">
    <h2>Live Video Classifier</h2>
    <p id="label-display">Label: <span id="label">waiting...</span></p>
  </section>
 
 <section id="about">
  <div class="container">
    <h2>Project Statements</h2>
     <div class="buttons">
      <button class="profile-btn" onclick="showDescription('joan')">Joan Lee</button>
      <button class="profile-btn" onclick="showDescription('madison')">Madison Sveum</button>
      <button class="profile-btn" onclick="showDescription('ann')">Ann Teoh</button>
    </div>
     <div id="description">
      <p id="joan" class="description">As we worked on training our own image recognition model using Google’s Teachable Machine, I started to notice something unsettling, not just in the way the model reacted to our inputs, but in what that reaction showed. The process made it clear how dependent AI is on the data it's given, and how easily it can fail when that data lacks variety or context. At the same time, reading Unmasking AI added a whole new layer to what we were doing. Her experiences with biased AI systems and the broader structural issues behind them helped me see that this isn’t just about technology. It’s about who gets represented, who doesn’t, and how that affects the outcomes AI produces. Even though our project was small, it made me start questioning the systems around me and the trust we put in algorithms that are, at their core, shaped by human choices.
Our goal for this project was to train an image classifier that could recognize three types of traffic signs: Stop, Yield, and Street. When we first started this project, I didn’t expect it to make me think so much about the real-world impact of AI. I thought we were just going to train a computer to recognize traffic signs using Google’s Teachable Machine, and that would be it. But as soon as we began collecting data and actually training our model, I realized there was a lot more going on behind the scenes. Every choice we made, from how we captured images to how consistent the lighting was, affected the results. It wasn’t just about the tech, but about how the tech learns. At the same time, we were reading Unmasking AI, and everything we learned from the book started to connect with what we were seeing firsthand. This project showed me how complicated and sensitive machine learning actually is, and how important it is to be thoughtful about it.
This project ended up being more than just a technical task. It gave us a chance to reflect on the bigger questions around machine learning, especially the concerns Joy Buolamwini raises in Unmasking AI. Even though our project was on a small scale, the same issues around bias, data quality, and accuracy were present throughout the process.
Originally, I planned to take our laptops outside and use the webcam on the teachable machine website to capture real images of traffic signs in our neighborhood. But I quickly ran into a problem. I realized that carrying the laptop around and trying to angle the webcam toward a street sign was awkward and not very effective since it is heavy and big. So instead, I came up with a workaround. I took photos of Stop, Yield, and Street signs, then pulled those photos up on our phone screens. To make the signs large enough and clear enough, I zoomed in and held our phones up to the webcam while capturing the training data.
This method actually worked better than I thought  in some ways. It allowed me to stay in one place and collect a decent number of images quickly. But it also created a new set of challenges. The model had a hard time recognizing the signs when there was glare on the phone screen or if the image wasn’t centered just right. Even slight differences in how I held the phones made the predictions less reliable. Sometimes, the model would completely misidentify a Stop sign as a Yield sign just because of a subtle shadow or angle shift.
After enough training images were collected, we tested the model to see how well it could recognize the signs in real time. The results were hit-or-miss. Occasionally it would work perfectly, but other times it would flicker between two labels.
This experience reminded us that machine learning is very sensitive to the conditions under which it’s trained. A model is only as good as the data it learns from. If it’s trained on a narrow set of examples (like screenshots of street signs on phone screens), it will struggle when it sees anything even slightly different.
As we worked on this, we kept thinking back to Unmasking AI and its content of experiences with biased facial recognition systems. One of the big takeaways from the book is that AI models can’t be trusted to be fair or accurate just because they’re based on data. The way data is collected, who is included or excluded, and what assumptions are built into the system. 

Even though we weren;t working with facial recognition, it struggled when the lighting changed or when it saw a sign it hadn't rained on. This might seem like a small issue, but it actually mirrors much bigger problems like how facial recognition systems often misidentify people with darker skins because they weren't well represented in the training data.
Looking back on the project, I think what surprised me most was how much it made me think—not just about how to train a machine, but about the bigger stuff that goes into AI. We started with this simple idea: let’s get a computer to recognize traffic signs. But the further we got, the more I realized how many little decisions we had to make, and how those decisions really shaped what the model could do. I also learned that building a model isn’t just about writing code, but it’s about making choices: what data to use, how to present it, how much to collect, and how to test it. Every step in that process affects the final result. It made me think more critically about the kinds of AI tools we encounter every day and how much trust I place in them.
I used to think of AI as this super advanced tech that just works. But doing this project showed me that it’s actually kind of fragile. Our model would work fine one second and then totally glitch out the next just because we moved the phone slightly or the lighting changed. And that’s just traffic signs—imagine that kind of instability in something serious like facial recognition or law enforcement.
Reading Unmasking AI while doing this made everything hit harder. The author talks a lot about bias in AI, and even though our project was small, we could already see how limited training data messes with the outcomes. 
I now know that every dataset, every decision, every tool we build reflects something. And going forward, I want to be the kind of person who pays attention to that and questions how things are made, not just if they work. </p>

        
      <p id="madison" class="description">I’m originally from Milwaukee, WI. I’m a junior studying Information Science. I’m minoring in Digital Studies, Digital Media Analytics, Sports Communication and Graphic Design. This semester in addition to LIS 500 Code and Power I’m taking LIS 472 Introduction to Web Development, LIS 640 Generative AI, JOURN 175 Digital Media Fluency, and ATM OCN 102 Climate Change.</p>
      <p id="ann" class="description">Hi, I’m Ann Teoh from Kuala Lumpur, Malaysia. I am a senior majoring in Information Science with a minor in Digital Studies at UW. I decided to take this course based on a recommendation from a friend who found it interesting. Initially, I enrolled to improve my web development skills, particularly in HTML and CSS. However, as the semester progressed, I found the course even more engaging than I expected. It not only deepens my technical skills but also connects to the ethical implications of technology—particularly how those in power influence technology and how issues of race and discrimination manifest in digital spaces. I want to become more aware of biases in technology and learn how to create more inclusive digital environments.</p>
    </div>
  </div>
</section>

<script>
  function showDescription(person) {
    document.querySelectorAll('.description').forEach(desc => desc.style.display = 'none');
    document.getElementById(person).style.display = 'block';
  }
</script>


  <script src="sketch.js"></script>

  <footer>
    <p>&copy; Joan Lee, Madison Sveum, Ann Teoh. LIS 500 Project2</p>
  </footer>


</body>
</html>
